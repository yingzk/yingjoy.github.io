---
title: 22. 对比最优误差率
date: 2018-05-05 15:19:31
en_title: Comparing to the optimal error rate
tags: [Machine Learning Yearning, Machine Learning, AI]
---

在我们的猫咪识别实例中，这个“想法”的错误率指的是——最优分类器的错误率接近0%，就像一个人可以很轻松的识别它。而且随时可以进行识别，我们希望机器也可以做到这点。
还有一些问题是比较困难的。例如：假设你建立了一个语音识别系统，并且发现有14%的音频杂音非常多，即使一个人也很难听出音频中在说什么。在这种情况下，这个“最优的”语音识别系统的误差大约为14%。
假设在这个语音识别系统中，你的算法效果如下：
• 在训练集上的误差 = 15%
• 在开发集上的误差 = 30%
在训练集上的效果接君最优误差14%。因此，在偏差和训练集上面进行改进是不会取得太大的效果的。然而这个算法并不适用于开发集；因此，由于方差的原因，在这里有很大的改进空间。
这个例子于上一章节的第三个例子类似，它有在训练集上有15%的误差，在开发集上有30%的误差。如果最优分类器的误差接近于0%的话，则训练集上有15%的误差改进空间非常大，减少偏差是非常有效的。但是如果最优错误率约为14%，那么近乎相同的训练集的数据告诉我们我们分类器是很难提高的。
对于最优错误率远大于0%的问题，这里有一个关于算法错误的更详细的分类。我们继续使用上面的语音识别示例，可以按如下方式分解在开发集上的30%误差。（在测试集上也可以类似进行错误分析）
• 最优误差率 (“不可避免的偏差”): 14%. 假设我们认为，即使世界上最好的语言我们仍会有14%的误差，我们可以把这个看作为不可避免的部分。
• 可避免的偏差 : 1%.由训练集上的误差于最优误差的差值计算得到。3
• 方差 : 15%.训练集与开发集上误差的区别。
由我们之前的定义，我们定义这两者关系如下：4
偏差 = 最优误差（不可避免的偏差） + 可避免的偏差
这个可避免的偏差反映了你算法的在训练集上与最优分类器直接的差别。
方差的定义与之前的定义一样，从理论上讲，我们可以通过对大量训练集的训练，将方差减少到接近0%的水平。因此，如果数据量足够大，所有的方差都是可避免的，反之不可避免。

再思考一个例子：假设最佳错误率为14%，我们有如下：
• 在训练集上的误差 = 15%
• 在开发集上的误差 = 16%
在前一章，我们把它叫做高偏差分类器，我们现在可以知道，可避免的误差为1%，误差在1%左右，因此，这个算法已经做的很好了，几乎没有改进空间，它与最佳错误率相差不超过2%。
我们从这些例子中可以看出，知道这个误差率有助于指导我们进行下一步工作。在统计中，最优错误率也称为贝叶斯误差率（Bayes error rate）或贝叶斯率。
我们怎么知道最优错误率是多少？对于人类擅长的任务，例如识别图片或视频剪辑，人类可以给结果打标签，然后计算出准确率。这里将给出最优错误率的一个评估准则。如果你在做一个人类做起来比较困难的事情（如：预测推荐电影，或者广告给一个用户）这就很难给出最优的错误率。
在“与人类性能比较[Comparing to Human-Level Performance]（33-35章节）”中，我们将详细讨论它。将学习算法的性能与人类的水平进行比较。
在最后的几章中，你会了解如何通过查看训练集和开发集的错误率来评估可避免/不可避免的偏差和方差。下一章将讨论如何利用这样的分析来区分偏差和一些其它的减小偏差的技术。根据你当前项目的情况，是高偏差（可避免的）还是高方差，你应该使用不同的优化方法。加油！

3 如果是负数，那么在训练集上你做得比最佳错误率更好。这意味着你过拟合了，此时你应该关注如何减少方差，而不是进一步的减少偏差。
4 这些定义是用于你算法改进的一些见解。这不同于统计学上定义的偏差和方差。从技术上讲，我们这里定义的偏差为，我在这里定义的“我们认为存在的错误”；而“可避免的偏差”应该是我们认为算法的错误率大于最佳错误率。