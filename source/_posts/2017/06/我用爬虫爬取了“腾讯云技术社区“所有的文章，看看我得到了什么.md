---
title: 我用爬虫爬取了“腾讯云技术社区“所有的文章，看看我得到了什么
date: 2017-06-19 16:25:03
en_title: Spider QCloud all blog and analyse
tags: [Spider, Data Analyse]
---

### 我用爬虫爬取了“腾讯云技术社区”所有的文章，看看我得到了什么

​

#### 前言

闲来周末练习下爬虫

就拿[腾讯云技术社区](https://www.qcloud.com/community)来开刀， 哈， 经典皮卡丘开头

![](//blog-10039692.file.myqcloud.com/1510467430155_5416_1510467431143.jpg)

​

这次我通过利用Python爬虫

加上一个"不完美"的分词系统

构建了，腾讯云技术社区所有文章的词云，来看看总体大概都写了什么

嘻嘻嘻:)

​

#### 正文

​

**编程思路**

1. 获取所有文章的地址
2. 对单文章页进行内容提取
3. 将所有文章进行内容提取，并将结果存入MongoDB数据库中
4. 利用分词系统和wordcloud进行词云的构建

​

**注:**存储所有文章地址前，我加了一个随机数，后期随机抽取文章进行提取

防止因日期不同导致结果具有局部性

​

​

##### 获取文章列表页，所有的文章信息

保存格式为: 

- index           随机数索引
- title             文章名
- address       文章地址 
- content        文章内容

​

```
    def get_one_page_all(self, url):
        try:
            html = self.get_page_index(self.baseURL)
			# 采用BeautifulSoup解析
            soup = BeautifulSoup(html, 'lxml')
            title = soup.select('.article-item > .title')
            address = soup.select('.article-item > .title > a[href]')
            for i in range(len(title)):
			# 生成随机索引
                random_num = random.randrange(0, 6500)
                content = self.parse_content('https://www.qcloud.com' + address[i].get('href').strip())
                yield {
                    'index' : random_num,
                    'title':title[i].get_text().strip(),
                    'address' : 'https://www.qcloud.com' + address[i].get('href').strip(),
                    'content' : content
                }
		# 遇到索引错误时跳过
        except IndexError:
            pass
```

​

##### 解析文章内容

```
    def parse_content(self, url):
        html = self.get_page_index(url)
        soup = BeautifulSoup(html, 'lxml')
	    #这里直接用了class为J-article-detail的div里面的内容
        content = soup.select('.J-article-detail')
        return content[0].get_text()
```

​

#### 结果

这里我就直接把最后生成的结果放出来了

由于分词系统不是很好，导致结果不是很理想

这里我利用了正则表达式，将内容中所有非中文的字符去掉了

​

**由于个人计算机配置不是很好，我将结果分为了20份，每份均为随机选取的100篇文章组成**

​

![](//blog-10039692.file.myqcloud.com/1510468438622_3620_1510468439803.png)

![](//blog-10039692.file.myqcloud.com/1510468447656_9113_1510468448677.png)

![](//blog-10039692.file.myqcloud.com/1510468453414_8137_1510468454434.png)

![](//blog-10039692.file.myqcloud.com/1510468459320_8412_1510468460370.png)

![](//blog-10039692.file.myqcloud.com/1510468465029_1643_1510468466043.png)

![](//blog-10039692.file.myqcloud.com/1510468471624_7061_1510468472657.png)

![](//blog-10039692.file.myqcloud.com/1510468481062_9876_1510468482067.png)

![](//blog-10039692.file.myqcloud.com/1510468486931_4842_1510468488025.png)

![](//blog-10039692.file.myqcloud.com/1510468492936_1261_1510468493930.png)

![](//blog-10039692.file.myqcloud.com/1510468498888_9789_1510468499885.png)

![](//blog-10039692.file.myqcloud.com/1510468503687_3171_1510468504724.png)

![](//blog-10039692.file.myqcloud.com/1510468509640_8061_1510468510575.png)

![](//blog-10039692.file.myqcloud.com/1510468548100_1319_1510468549087.png)

![](//blog-10039692.file.myqcloud.com/1510468555316_7035_1510468556312.png)

![](//blog-10039692.file.myqcloud.com/1510468562975_4077_1510468563925.png)

![](//blog-10039692.file.myqcloud.com/1510468569871_3351_1510468570844.png)

![](//blog-10039692.file.myqcloud.com/1510468579010_7830_1510468579990.png)

![](//blog-10039692.file.myqcloud.com/1510468585472_6_1510468586505.png)

![](//blog-10039692.file.myqcloud.com/1510468592068_4884_1510468593117.png)

![](//blog-10039692.file.myqcloud.com/1510468599025_2702_1510468600032.png)

​

这就是所有文章生成的词云，分词和筛选不是很好，导致数词、人称名词多

​

#### 总结

可以看出， 腾讯云技术社区上的文章，大部分都是和数据有关的

​

哈哈，不是很理想，待日后改善一下（词的筛选）

​

最后打个小广告，希望大家关注下我的公众号: [ikang\_kj](https://yingjoy.cn/gzh.jpg)

嘿嘿 :)